{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vllm serve microsoft/Phi-3.5-mini-instruct --dtype auto --max-model-len 8000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vllm_server_url = \"http://localhost:8000/v1/chat/completions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(prompt, model=\"microsoft/Phi-3.5-mini-instruct\"):\n",
    "    \n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    \n",
    "    data = {\n",
    "        \"model\": model,\n",
    "        \"messages\": messages,\n",
    "        \"n\": 1,\n",
    "        \"max_tokens\": 1000,\n",
    "        \"temperature\": 0.0,\n",
    "        \"top_p\": 0.9,\n",
    "        \"top_k\": 40,\n",
    "        \"stop\": [\"<|endoftext|>\"],\n",
    "        \"stream\": False\n",
    "    }\n",
    "    \n",
    "    response = requests.post(vllm_server_url, headers=headers, data=json.dumps(data))\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        return f\"Error: {response.status_code}, {response.text}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"chatcmpl-5eab6ccf5ce64527a62edaade37bab03\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1735295500,\n",
      "  \"model\": \"microsoft/Phi-3.5-mini-instruct\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \" Ram is the father of Shyam. The relationship between Ran and Shyam is not explicitly mentioned in the given context. If \\\"Ran\\\" refers to another person, there is no direct familial connection provided between Ran and Shyam based on the given information. To clarify the relationship, additional context or information would be required.\",\n",
      "        \"tool_calls\": []\n",
      "      },\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"stop_reason\": 32007\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 60,\n",
      "    \"total_tokens\": 130,\n",
      "    \"completion_tokens\": 70,\n",
      "    \"prompt_tokens_details\": null\n",
      "  },\n",
      "  \"prompt_logprobs\": null\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Based on the context below answer the user query that follows:\n",
    "Ram is the father of shyam and shayam has a brother krishna\n",
    "Query: What is relation between ran and shyam?\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "result = generate_response(prompt)\n",
    "print(json.dumps(result, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "\n",
    "# response = requests.post(\"http://127.0.0.1:8080/predict\", json={\"query\": \"What is DSpy?\"})\n",
    "# print(f\"Status: {response.status_code}\\nResponse:\\n {response.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/david/anaconda3/envs/r_apirag/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-12-27 11:33:08,102\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-27 11:34:26 config.py:131] Replacing legacy 'type' key with 'rope_type'\n",
      "INFO 12-27 11:34:28 config.py:510] This model supports multiple tasks: {'classify', 'reward', 'score', 'generate', 'embed'}. Defaulting to 'generate'.\n",
      "INFO 12-27 11:34:28 llm_engine.py:234] Initializing an LLM engine (v0.6.6.post1) with config: model='microsoft/Phi-3.5-mini-instruct', speculative_config=None, tokenizer='microsoft/Phi-3.5-mini-instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8000, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=microsoft/Phi-3.5-mini-instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"candidate_compile_sizes\":[],\"compile_sizes\":[],\"capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "INFO 12-27 11:34:28 model_runner.py:1094] Starting to load model microsoft/Phi-3.5-mini-instruct...\n",
      "INFO 12-27 11:34:28 weight_utils.py:251] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  2.32it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.84it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-27 11:34:30 model_runner.py:1099] Loading model weights took 7.1176 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-27 11:34:30 worker.py:241] Memory profiling takes 0.69 seconds\n",
      "INFO 12-27 11:34:30 worker.py:241] the current vLLM instance can use total_gpu_memory (23.55GiB) x gpu_memory_utilization (0.90) = 21.19GiB\n",
      "INFO 12-27 11:34:30 worker.py:241] model weights take 7.12GiB; non_torch_memory takes 0.01GiB; PyTorch activation peak memory takes 0.51GiB; the rest of the memory reserved for KV Cache is 13.55GiB.\n",
      "INFO 12-27 11:34:30 gpu_executor.py:76] # GPU blocks: 2313, # CPU blocks: 682\n",
      "INFO 12-27 11:34:30 gpu_executor.py:80] Maximum concurrency for 8000 tokens per request: 4.63x\n",
      "INFO 12-27 11:34:31 model_runner.py:1415] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:09<00:00,  3.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-27 11:34:40 model_runner.py:1535] Graph capturing finished in 9 secs, took 0.40 GiB\n",
      "INFO 12-27 11:34:40 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 10.89 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_name = \"microsoft/Phi-3.5-mini-instruct\"\n",
    "llm = LLM(model=model_name, max_model_len=8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.81s/it, est. speed input: 65.89 toks/s, output: 93.32 toks/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Based on the provided context information, DSPy (presumably a software or programming system) utilizes natural language signatures as a method to guide and refine the work given to a Language Model (LM). These signatures are essentially concise, declarative statements that clearly define the function or transformation required for a particular text. For example, a signature could be, \"Consume questions and return answers,\" which communicates the desired outcome without dictating the specific approach an LM should take.\\n\\nThe structure of a DSPy signature is composed of tuples, where each tuple contains pairs of input fields and output fields, with an optional instruction field. Each field in the tuple has a name and may include optional metadata. This structure allows for a clear and structured communication of tasks between the DSPy system and the LM, enabling more efficient and effective prompting and fine-tuning processes.\\n\\nIn summary, DSPy signatures serve as a sophisticated tool for abstracting the prompting process, reducing reliance on free-form strings while providing a clear, natural language specification for text transformations. This approach facilitates better coordination between the system and the Language Model, ultimately improving the quality and accuracy of the generated results.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"Context information is below.\\n---------------------\\nPreprint\\n3.1 N ATURAL LANGUAGE SIGNATURES CAN ABSTRACT PROMPTING &FINETUNING\\nInstead of free-form string prompts, DSPy programs use natural language signatures to assign work\\nto the LM. A DSPy signature is natural-language typed declaration of a function: a short declarative\\nspec that tells DSPy what a text transformation needs to do (e.g., “consume questions and return\\nanswers”), rather than how a specific LM should be prompted to implement that behavior. More\\nformally, a DSPy signature is a tuple of input fields andoutput fields (and an optional instruction ).\\nA field consists of field name and optional metadata.\"\n",
    "\n",
    "\n",
    "from vllm.sampling_params import SamplingParams\n",
    "\n",
    "\n",
    "messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "\n",
    "sampling_params = SamplingParams(max_tokens=8192, temperature=0.7)\n",
    "outputs = llm.chat(messages=messages, sampling_params=sampling_params)\n",
    "outputs[0].outputs[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kr_apirag",
   "language": "python",
   "name": "kr_apirag"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
